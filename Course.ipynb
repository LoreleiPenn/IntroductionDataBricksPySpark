{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, Column, Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from typing import Dict, List\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/29 11:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWords(df: DataFrame, words: List[str],\n",
    "            columns: List[str], outputColumn: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Extract the list of `words` from `columns` from the table `df` and\n",
    "    output them into `outputColumn`.\n",
    "    \"\"\"\n",
    "    # Convert the list of `words` into a string separated with pipes.\n",
    "    regexList = \"|\".join(words)\n",
    "    # Add parenthesis to the regexList so that they turn into a group.\n",
    "    regexPattern = '(' + regexList + ')'\n",
    "    # Define a local function to concat the columns `columns`.\n",
    "    def concatInterestColumns(columns: List[str]) -> Column:\n",
    "        # Define a separator.\n",
    "        separator = '|'\n",
    "        # Map the columns into a list of F.col() objects.\n",
    "        columnsSpark = list(map(F.col, columns))\n",
    "        # Return the concatenated columns as a concatenated column.\n",
    "        return F.concat_ws(separator, *columnsSpark)\n",
    "    # Define a local function to extract the `regexPattern` from `column`.\n",
    "    def regexExtract(column: Column) -> Column:\n",
    "        return F.regexp_extract(column, regexPattern, 0)\n",
    "    # Define a composition of functions to concatenate and extract from\n",
    "    # the concatenation to prevent the usage of multiple `.withColumn`.\n",
    "    def extractFromConcatenatedColumns(columns: List[str]) -> Column:\n",
    "        return regexExtract(concatInterestColumns(columns))\n",
    "    # Apply the previous concatenation and name it with `outputColumn`. \n",
    "    return df.withColumn(outputColumn, extractFromConcatenatedColumns(columns))\n",
    "\n",
    "\n",
    "def standardizeWords(df: DataFrame, standardDict: Dict[str, List[str]],\n",
    "                    column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize the values within the column `column` from the table\n",
    "    `df`, according to the information from `standardDict`.\n",
    "    \"\"\"\n",
    "    # Start by inverting the dictionary so that for each element of the\n",
    "    # list, it will standardize to the key.\n",
    "    invertedDict = {value: key for (key, values) in standardDict.items()\n",
    "                               for value in values}\n",
    "    # Start the standardization query\n",
    "    standardizeQuery = \"CASE \"\n",
    "    # Dynamically generate the standardize query according to the values\n",
    "    # from the inverted dictionary `invertedDict`.\n",
    "    for (key, value) in invertedDict.items():\n",
    "        standardizeQuery += f\"WHEN {column} RLIKE '{key}' THEN '{value}' \"\n",
    "    # Finalize the standardization query.\n",
    "    standardizeQuery += f\"ELSE {column} END\"\n",
    "    # Evaluate the dictionary with `F.expr` so that we only use once the\n",
    "    # evaluation from `withColumn` and optimize the process.\n",
    "    print(standardizeQuery)\n",
    "    return df.withColumn(column, F.expr(standardizeQuery))\n",
    "\n",
    "\n",
    "def extractAndStandardizeWords(df: DataFrame, columns: List[str],\n",
    "                               outputColumn: str,\n",
    "                               stdDict: Dict[str, List[str]]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize the words `words` which are extracted from the columns\n",
    "    `columns` over the table `df` by using the information from the\n",
    "    standardization dictionary `standardDict`.\n",
    "    \"\"\"\n",
    "    # Generate the words from the values of the standardize Dictionary.\n",
    "    words = reduce(lambda list1, list2: list1 + list2, stdDict.values())\n",
    "    # First extract the words using the `extractWords` function.\n",
    "    extractedWords = extractWords(df, words, columns, outputColumn)\n",
    "    # Next standardize the words using the `standardizeWords` function.\n",
    "    standardizedWords = standardizeWords(extractedWords, standardDict,\n",
    "                                         outputColumn)\n",
    "    # Return the object in which the words were extracted and standardized.\n",
    "    return standardizedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filepath: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the data from `filepath` and process it with the options for\n",
    "    this specific project. Return a Spark DataFrame object containing\n",
    "    its data.\n",
    "\n",
    "    Parameters:\n",
    "        filepath: str\n",
    "            This is the path to the CSV containing the data.\n",
    "    \"\"\"\n",
    "    options = {\n",
    "        \"header\": True,\n",
    "        \"inferSchema\": True,\n",
    "        \"nullValues\": None\n",
    "    }\n",
    "    return spark.read.options(**options).csv(filepath)\n",
    "\n",
    "\n",
    "def processColumnsUpper(df: DataFrame, columns: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transform the columns `columns` from the table `df` to upper case. \n",
    "    \"\"\"\n",
    "    layout = df.columns\n",
    "    def upperCaseColumn(layout: List[str], column: str) -> Column:\n",
    "        return (F.upper(column).alias(column) if column in layout\n",
    "                                              else F.col(column))\n",
    "    def upperCaseCols(layout: List[str], columns: List[str]) -> List[Column]:\n",
    "        return list(map(lambda column: upperCaseColumn(layout, column), columns))\n",
    "    newLayout = upperCaseCols(layout, columns)\n",
    "    return df.select(newLayout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"rating\", \"description\"]\n",
    "colsSpark = list(map(F.col, cols))\n",
    "dfUpper = processColumnsUpper(df, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE WHEN test RLIKE '(^|\\W)NFL(\\W|$)' THEN 'TEST' WHEN test RLIKE 'TEAMMATES' THEN 'TEST' WHEN test RLIKE 'MISSION' THEN 'TEST' ELSE test END\n",
      "+------+--------------------+-----+\n",
      "|rating|         description| test|\n",
      "+------+--------------------+-----+\n",
      "|     R|NEW NFL STAR THAD...| NFL |\n",
      "|     R|BAD NEWS FROM THE...| TEST|\n",
      "| TV-MA|IN THIS DARK COME...| TEST|\n",
      "| TV-MA|WHEN A PECULIAR H...| TEST|\n",
      "| TV-14|FIVE FRIENDS EMBA...| TEST|\n",
      "| PG-13|AN FBI AGENT MAKE...| TEST|\n",
      "| TV-PG|THIS DOCUMENTARY ...| TEST|\n",
      "|     R|AFTER FAKING HIS ...| TEST|\n",
      "| TV-MA|STAND-UP COMEDIAN...| NFL |\n",
      "| TV-14|A REBELLIOUS DAUG...| TEST|\n",
      "| TV-MA|ARMED WITH MYSTER...| TEST|\n",
      "| TV-MA|IN EACH EPISODE O...| TEST|\n",
      "| TV-MA|WHEN A YOUNG GIRL...| TEST|\n",
      "| TV-MA|FROM CRIPPLING PA...| TEST|\n",
      "|  TV-Y|OH TAKES IT UPON ...| TEST|\n",
      "|    PG|SHE’S NOT THE USU...| TEST|\n",
      "| TV-14|DRIVEN BY THE LES...| TEST|\n",
      "| TV-14|AFTER HER HUSBAND...| TEST|\n",
      "|     R|A HARDENED MERCEN...| TEST|\n",
      "| TV-PG|HÜSEYIN HAS FOUND...| TEST|\n",
      "+------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standardDict = {\n",
    "    \"TEST\": [\"(^|\\W)NFL(\\W|$)\", \"TEAMMATES\", \"MISSION\"]\n",
    "}\n",
    "extractAndStandardizeWords(dfUpper, cols, \"test\", standardDict).filter(F.col(\"test\") != \"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', 'x', 'y']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\n",
    "    \"a\": [\"1\", \"2\"],\n",
    "    \"b\": [\"x\", \"y\"]\n",
    "}\n",
    "reduce(lambda a, b: a + b, a.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'a', '2': 'a', 'x': 'b', 'y': 'b'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{val: key for (key, value) in a.items() for val in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
